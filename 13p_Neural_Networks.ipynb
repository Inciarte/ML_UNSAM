{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13p_Neural_Networks",
      "provenance": [],
      "collapsed_sections": [
        "aoNOblmHX1XY",
        "P_staGAxY9yH",
        "36PryIS5X_Oc",
        "jojgQMNjYFQu",
        "TBYZ4zFYYHDG",
        "9OwGQ8puZJmI",
        "aRf0_MB6ZdBX",
        "6zMsHUuRZgBj",
        "DpzyBn70ZigS",
        "FhizHlVnkjKR",
        "7hjgiSLzbRo1",
        "JqVrGNsqlMYU",
        "RGzHSPB1rm3V",
        "Eavej6KrwCWg",
        "Hn-CCGOumrj8",
        "8o_GcI93bC3-",
        "j_DUVnD2bFGb"
      ]
    },
    "kernelspec": {
      "display_name": "Python [conda env:tf]",
      "language": "python",
      "name": "conda-env-tf-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC18EFXdSqmd"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#The stars\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuLqMAEqS5fa"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-BToGrRTCK1"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTr73m2gTDu2"
      },
      "source": [
        "tf.config.list_physical_devices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoNOblmHX1XY"
      },
      "source": [
        "# Datos: Clasificacion de ropa (Fashion_MNIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PMm5jBHq_7a"
      },
      "source": [
        "Asi como sklearn trae utilidades para cargar datasets estandar, keras también trae. En general, keras puede aceptar datasets en forma de Numpy Arrays (como sklearn), pero también trae una clase Dataset que esta optimizada para cargar datos (incluso si son mas grandes que la memoria RAM del equipo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiVqgSJhTeXp"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy7mdQSCgIWs"
      },
      "source": [
        "Veamos el shape y el tipo de datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjrBwOsAgKde"
      },
      "source": [
        "print(X_train_full.shape,y_train_full.shape)\n",
        "print(X_train_full.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42tIWD0xgoGd"
      },
      "source": [
        "Tenemos la imagen en (28,28) y cada pixel es un entero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIus71mjtqxf"
      },
      "source": [
        "En Datasets grandes, CrossValidation puede ser muy costoso, por lo cual se recomienda separar un conjunton de validación aparte del training. Esto hacemos a continuación.\n",
        "\n",
        "También normalizamos los píxeles (que van de 0 a 255) para que estén entre 0 y 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cB3KYyaUnzP"
      },
      "source": [
        "#Separo en entrenamiento y validacion, y normalizo los pixeles\n",
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I16qmEbDVnsa"
      },
      "source": [
        "print(X_train_full.shape)\n",
        "\n",
        "plt.imshow(X_train[0], cmap='binary', interpolation='bicubic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nY8YQ_0uEPg"
      },
      "source": [
        "Los targets son numericos, del 0 al 9. Podemos guardar las etiquetas asi nos es mas facil analizar que tan bien o mal funciona nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNkEq7xEU1YD"
      },
      "source": [
        "class_names = [\"Remera/top\", \"Pantalones\", \"Pullover\", \"Vestido\", \"Abrigo\",\n",
        "               \"Sandalias\", \"Camisa\", \"Zapatillas\", \"Bolso\", \"Bota\"]\n",
        "\n",
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_staGAxY9yH"
      },
      "source": [
        "# Definir un modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsfK9opSuPfg"
      },
      "source": [
        "Hay tres formas de definir modelos:\n",
        "* Una es utilizando la clase de ``keras.models.Sequential``, que es apta para modelos donde solamente Layers adyacentes se encuentran conectados entre sí. \n",
        "* Otra es API funcional, que provee mucha mas flexibilidad, ya que se puede construir cualquier clase de red dirigida acíclica. \n",
        "* La ultima forma, es con SubClassing, es decir definiendo nuestras propias clases que hereden de las definidas en Keras, lo cual nos permite extender Keras para nuestras necesidades específicas, aunque no es recomendable a menos que realmente sepamos lo que estamos haciendo.\n",
        "\n",
        "Si bien ahora quizás les sea mas sencillo utilizar la API secuencial, aprender a utilizar la API funcional es casi igual de fácil y provee mas flexibilidad, asi que es la forma que recomiendo de aprender a usar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36PryIS5X_Oc"
      },
      "source": [
        "## API Secuencial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZXmZK8yXF4u"
      },
      "source": [
        "### Modo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3lSxEA-vSsr"
      },
      "source": [
        "Podemos crear un modelo, e ir añadiendo Layers sobre la marcha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvgn-wSlYV89"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBL-AitfskZy"
      },
      "source": [
        "28**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnwg1ZjvXLZE"
      },
      "source": [
        "#Creo un modelo secuencial\n",
        "model = keras.models.Sequential()\n",
        "#Y le voy agregando layers\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28])) #input layer #matriz->vector (como np.ravel)\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\")) #hidden 1 #784*300+300 = 785*300 = 235500\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\")) #hidden 2 #300*100+100=301*100=30100\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\")) #output #100*10+10=10100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sGF3s9xsHpH"
      },
      "source": [
        "Las funciones de activacion son muy importantes a la hora de usar DNN. Son las que agregan no-linearidad a la situacion.\n",
        "\n",
        "En las capas internas, la mas usual es ReLU, una funcion que es 0 si $z\\leq0$ y z si $z>0$. Hay otras opciones que suelen ser mejores, en particular para datasets grandes y con ciertas condiciones, pero ReLU tiene mucha ventaja en velocidad y optimizacion de librerias.\n",
        "\n",
        "En la capa externa, va a depender del problema.\n",
        "\n",
        "\n",
        "*   Para clasificacion, se utilizan sigmoide o softmax.\n",
        "*   Para regresion, si no tenemos ninguna restriccion no utilizamos ninguna funcion de activacion. Si queremos $y\\geq 0 $, podemos usar una funcion ReLu o una version mas suavizada llamada Softplus. Si queremos $a\\leq y \\leq b$, se puede utilizar una funcion sigmoide o tangente hiperbolica y re-escalearla.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXkcO1-B5rmc"
      },
      "source": [
        "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jojgQMNjYFQu"
      },
      "source": [
        "### Modo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb17UrHJvTv8"
      },
      "source": [
        "También podemos directamente pasar la lista de Layers al constructor del Modelo ``Sequential``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63vXIjKCYTOm"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_2k6HdfXlr5"
      },
      "source": [
        "#Sino directamente le paso una lista al crearlo:\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBYZ4zFYYHDG"
      },
      "source": [
        "## API Funcional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gWy2OctvbZF"
      },
      "source": [
        "La API funcional funciona definiendo Layers y utilizandolo como si fueran \"funciones\" que se aplican a los outputs de otros Layers. Keras irá rastreando estas conecciones, y luego generará un modelo entre el Layer de Inputs, y el de Outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BdBiQp1YXiJ"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-1uPqZmYIeh"
      },
      "source": [
        "input_ = keras.layers.Input(shape=[28, 28]) #En este caso es necesario definir el Layer de Inputs\n",
        "flatten = keras.layers.Flatten()(input_)\n",
        "hidden1 = keras.layers.Dense(300, activation=\"relu\")(flatten)#235500\n",
        "hidden2 = keras.layers.Dense(100, activation=\"relu\")(hidden1)# \n",
        "output = keras.layers.Dense(10, activation=\"softmax\")(hidden2)#charlemos un poquito la softmax despues\n",
        "model = keras.models.Model(inputs=[input_], outputs=[output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OwGQ8puZJmI"
      },
      "source": [
        "# Compilar y visualizar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1n6UL-ovs4G"
      },
      "source": [
        "Una vez que hemos definido nuestro modelo, es necesario que lo \"compilemos\", entonces Keras creará el gráfico computacional en TensorFlow de acuerdo a como lo hemos definido. Al compilar necesitamos especificar la funcion de perdida, el optimizador y las distintas metricas que querramos almacenar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVncsjmrZMbz"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vOu_s_zhNel"
      },
      "source": [
        "La funcion de perdida define el problema y esta intimamente relacionada con la funcion de activacion de la output layer. Para clasificacion, tenemos algunas sutilezas.\n",
        "\n",
        "Si tenemos un problema de dos clases, tenemos dos opciones:\n",
        "\n",
        "* Utilizamos una unica neurona en la output layer. Esa neurona modela la asignacion a clase 1 o clase 2. La funcion de activacion sera entonces una sigmoide y la funcion de perdida la binary_cross_entropy.\n",
        "* Utilizamos dos neuronas en la output layer. Cada una modela la probabilidad de pertenecer a una determinada clase. Dado que deben sumar 1, utilizamos la funcion de activacion softmax.  La funcion de perdida sera categorica. Pero aun hay mas. Si utilizamos la representacion $t={0,1}$, utilizamos sparse_categorical_cross_entropy. Si utilizamos $t={[0,1],[1,0]}$, utilizamos simplemente categorical_cross_entropy. Se puede pasar de sparse a categorical con `tensorflow.keras.utils.to_categorical` y de categorical a sparse con `np.argmin()`.\n",
        "\n",
        "Cuando tenemos N clases, tenemos lo siguiente:\n",
        "\n",
        "*   Si tenemos un problema MultiClase, en el que queremos asignar la medicion a una de las clases, utilizamos funcion de activacion softmax y categorical_cross_entropy o sparse_categorical_cross_entropy dependiendo de como sea $t$.\n",
        "*   Si tenemos N clasificaciones en donde solo queremos decir si pertenece o no a determinada clase, utilizamos funcion de activacion sigmoide y cateogircal_cross_entropy (con el one hot encoding).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zARoy-X0v66Q"
      },
      "source": [
        "Podemos acceder a los diferentes Layers, mediante el atributo ``.layers``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaYiz7qVZqSw"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYy-e7pzwBHz"
      },
      "source": [
        "Y También podemos imprimir un resumen de nuestro modelo, asi como un lindo grafico de el:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LXinQM0ZquT"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HugLm6sZvmx"
      },
      "source": [
        "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcFeksoHwHbQ"
      },
      "source": [
        "Cada layer tiene atributos a los que podemos acceder, como el nombre, y métodos que nos devuelven, por ejemplo, los pesos.\n",
        "\n",
        "Vemos que por default inicializa los pesos de forma aleatoria entre 0 y 1, y los bias a cero. Esto se puede cambiar pasando un ``initializer=``al compilar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Js3Y9CgZ17S"
      },
      "source": [
        "hidden1 = model.layers[2]\n",
        "print(hidden1.name)\n",
        "weights, biases = hidden1.get_weights()\n",
        "print(weights.shape)\n",
        "#plt.hist(weights[0])\n",
        "#print(weights[0], biases[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRf0_MB6ZdBX"
      },
      "source": [
        "# Entrenar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRSYAHruwcjM"
      },
      "source": [
        "Entrenar es tan facil como hacer un ``.fit``, donde podemos especificar muchas cosas como épocas, datos de validación (que evaluará al final de cada época), métricas, etc.\n",
        "\n",
        "El método devuelve un objeto de Historia, con toda la información del entrenamiento (en forma de diccionarios), que podremos utilizar para analizar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL9AKbgfZeQe"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubNbhHnEwwCq"
      },
      "source": [
        "Examinemos el diccionario de parámetros del fit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IITIPuw9aATd"
      },
      "source": [
        "history.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgFx-tzNaGxG"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tg1GkrJw11Y"
      },
      "source": [
        "Podemos usar el atributo \"history\" que nos da un diccionario, para plotear las métricas que usamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ksDH-nWaLfX"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zMsHUuRZgBj"
      },
      "source": [
        "# Evaluar y predecir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0RQHI_GxEw0"
      },
      "source": [
        "Evaluar en un conjunto de Test, o hacer predicciones, es sumamente fácil:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYcvFqohZhVv"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npHqXo7aSUs"
      },
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lysdDpXXaXFY"
      },
      "source": [
        "y_pred = model.predict_classes(X_new)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ybn92SmonYj"
      },
      "source": [
        "y_pred = np.argmax(y_proba, axis=1)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVDFbrnOaXN6"
      },
      "source": [
        "np.array(class_names)[y_pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffbf9KFtadJY"
      },
      "source": [
        "plt.figure(figsize=(7.2, 2.4))\n",
        "for index, image in enumerate(X_new):\n",
        "    plt.subplot(1, 3, index + 1)\n",
        "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[y_test[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpzyBn70ZigS"
      },
      "source": [
        "# Guardar y Cargar Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S__NXAnsxI_5"
      },
      "source": [
        "Guardar modelos es tan sencillo como hacer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDg65TpBZkkH"
      },
      "source": [
        "model.save(\"my_keras_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyTFAQroxL2B"
      },
      "source": [
        "Que luego se puede cargar (por ejemplo en otra computadora, con otro script, para hacer predicciones) usando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT-6iPoPauFY"
      },
      "source": [
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrejS892xUQ5"
      },
      "source": [
        "También podemos guardar solamente los weights de los layers. Pero en este caso deberemos, antes de cargarlos, definir el mismo modelo que usamos y recien entonces cargar los weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd0dte83axdC"
      },
      "source": [
        "model.save_weights(\"my_keras_weights.ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCEBS3enaz0H"
      },
      "source": [
        "model.load_weights(\"my_keras_weights.ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhizHlVnkjKR"
      },
      "source": [
        "# Regresión: California Housing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdlNX14NxgdO"
      },
      "source": [
        "Veamos como se desempeña para hacer regresión con el dataset de california housing que utilizamos en la guía de ejercicios Datasets. Importamos los datos, y los escaleamos usando StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_g1GZM9km8Q"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaLKkccNxptl"
      },
      "source": [
        "El modelo que definimos tiene como ultimo layer una funcion lineal, sin ninguna funcion de activación. (Es como hacer regresión lineal, pero fiteando unidades ReLu como funciones de base)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFDtfmzxkyls"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR4axGxy2MCS"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogy8zfnUk9lJ"
      },
      "source": [
        "pd.DataFrame(history.history).plot()\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0.3, 0.4)\n",
        "plt.show()\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5fQBF1XlAYM"
      },
      "source": [
        "## Ventajas de la API funcional: Modelos no secuenciales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKCoZmChx5rf"
      },
      "source": [
        "Aqui vemos un ejemplo de como podemos usar la API funcional para hacer modelos mas complejos. En el modelo [Wide and Deep](https://https://ai.google/research/pubs/pub45413) se conectan los inputs directamente a los outputs, ademas de pasando por una red profunda de dos capas. De esta manera, el modelo puede aprender features mas abstractos de los datos, y a la vez tomarlos directamente sin distorisión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emM5moUylGFE"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model = keras.models.Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJBY5y7fpWc2"
      },
      "source": [
        "keras.utils.plot_model(model, \"wide_and_deep_model.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmapksBIlNWA"
      },
      "source": [
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hjgiSLzbRo1"
      },
      "source": [
        "# EXTRAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0UQUJJOyVu0"
      },
      "source": [
        "Callbacks son una manera de hacer cosas durante el entrenamiento. Son utiles en caso que el modelo tarde mucho en fitear. Por ejemplo, podemos guardar el modelo al fin de cada epoca (por si se nos apaga la maquina en el medio del training), o implementar un regularizador EarlyStopping y dejar de fitear si el validation score no mejora, o enviarnos un mail cuando termine el entrenamiento con los resultados, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW0PRpE-a10X"
      },
      "source": [
        "## Callbacks: Guardar durante entrenamiento (checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtJ5vPqplluY"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nY_ql6qa5ph"
      },
      "source": [
        " checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
        "\n",
        " history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                     callbacks=[checkpoint_cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3OtrP40p__9"
      },
      "source": [
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpZxyWWqqCPQ"
      },
      "source": [
        "model.evaluate(X_valid,y_valid) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03oEvTcQbVcL"
      },
      "source": [
        "## Callbacks: Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWppr7b4lvms"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3bXGXaRbXIt"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmuTxeNCl6nc"
      },
      "source": [
        "## Busqueda de Hiperparámetros: Sklearn integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5nqSWlNytYA"
      },
      "source": [
        "Y lo mejor de todo, tiene un wrapper que nos permite integrarlo con ScikitLearn. Sklearn lo reconocerá el modelo como uno más de los suyos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db6zYIGMl5_B"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxMV0o_vmBDG"
      },
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vaBNv0-mWV3",
        "scrolled": false
      },
      "source": [
        "keras_reg.fit(X_train, y_train, epochs=30,\n",
        "              validation_data=(X_valid, y_valid),\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_sHKiYYy7O7"
      },
      "source": [
        "El modelo tiene los métodos usuales de un estimador de sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrpGmS1FmcSC"
      },
      "source": [
        "mse_test = keras_reg.score(X_test, y_test)\n",
        "y_pred = keras_reg.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUwX9oDNy2HY"
      },
      "source": [
        "Por ejemplo, podemos usarlo para buscar hiperparámetros con RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkr2t3Ivmnfz"
      },
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_valid, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxACOnKUft4e"
      },
      "source": [
        "Y accedemos a los resultados del CV como siempre hacemos en sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqry-VbMmtEB"
      },
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghtQ2174mukj"
      },
      "source": [
        "rnd_search_cv.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-JpwTtQmv-R"
      },
      "source": [
        "rnd_search_cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNPoFVMRmxQP"
      },
      "source": [
        "rnd_search_cv.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr3DHzRBmzdd"
      },
      "source": [
        "model = rnd_search_cv.best_estimator_.model\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajW_nTpNft4y"
      },
      "source": [
        "### Otras librerías para Optimización:\n",
        "\n",
        "Hay muchas, que hacen cosas un poco mejores que buscar en una grilla o de forma Random. Acá selecciono tres a dedo (en el Gerón pueden ver más opcionaes también):\n",
        "\n",
        " * [Keras Tuner](https://keras-team.github.io/keras-tuner/): Es parte del ecosistema de Keras, por lo que está optimizado para la versión de tensorflor: tf.keras, y tiene herramientas de visualización útiles.\n",
        " * [Scikit-Optimize](https://scikit-optimize.github.io/): Similar a sklearn para algoritmos de aprendizaje automatizado, es esta librería para algoritmos de optimización. En particular `BayesSearchCV` hace optimización Bayesiana con una interfaz similar al `GridSearchCV`.\n",
        " * [Sklearn-Deap](https://github.com/rsteca/sklearn-deap): Esta pretende imitar al `GridSearchCV` de sklearn para buscar hiperparámetros, pero utilizando algoritmos evolutivos en su lugar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqp8IMn_ft4y"
      },
      "source": [
        "# Regularización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yaDFBJrft4z"
      },
      "source": [
        "Sabemos como hacer más flexible nuestra red: Aumentamos el numero de Layers o de Neuronas por Layer. Pero, cómo la regularizamos para prevenir overfitting?\n",
        "\n",
        "Acá presentamos tres formas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO6qM7DAft40"
      },
      "source": [
        "## Weight-decay: Regularización $\\ell_2$ ó $\\ell_1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxOlPfO8ft41"
      },
      "source": [
        "Similar a los métodos de regularización Ridge y Lasso para modelos lineales, que añadían un término a la función de costo proporcional a la norma $\\ell_2$ o $\\ell_1$ del vector de pesos, uno puede hacer lo mismo en la red neuronal. \n",
        "\n",
        "Implementar weight decay en una red neuronal en keras, es tan simple como añadir el argumento `kernel_regularizer=` en los layers que queramos regularizar. Los regularizadores $\\ell_2$ o $\\ell_1$ los podemos encontrar en `keras.regularizers.l2()` y  `keras.regularizers.l1()`, o incluso un ElasticNet (combinacion de ambos) en `keras.regularizers.l1_l2()`. Al inicializarlos les deberíamos pasar como argumento el peso que tendrá estos términos (equivalente al `alpha` en sklearn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfM7HylOft42"
      },
      "source": [
        "#por ejemplo para definir un layer denso ReLy de 100 neuronas y regularización l2 (~Ridge) con alpha=0.01:\n",
        "layer = keras.layers.Dense(100,\n",
        "                          activation='relu',\n",
        "                          kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGtR3s41ft45"
      },
      "source": [
        "Gerón ademas nos da un truco para ahorrarnos escribir esto en cada layer, y cualquier hiperparámetro que se repita: Crear un wrapper a dicho layer con la función de Python `functools.partial`. Esta crea wrappers sencillos a funciones/objetos, que pasan hiperparámetros default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNJCOTDIft46"
      },
      "source": [
        "#Por ejemplo el modelo primero del notebook, pero regularizado:\n",
        "from functools import partial\n",
        "\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                          activation='relu',\n",
        "                          kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(300),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wveIngKOft4-"
      },
      "source": [
        "## Dropout Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYuzXI7Hft4_"
      },
      "source": [
        "Una forma muy popular de hacer regularización fue propuesta en dos papers de 2012 y 2014: En cada paso de entrenamiento, algunas neuronas de la red se \"apagan\" con una probabilidad $p$. Esto solo se hace durante entrenamiento (no durante test o en predicciones). La idea de fondo es que estamos forzando a la red a que no dependa fuertemente de ninguna neurona en particular, sino que sería dar mucha importancia a algún feature en particular, sino que la forzamos a dar resultados aún cuando ciertas neuronas se apagan.\n",
        "\n",
        "En keras, esto ya ha sido implementado como un Layer, así que solamente lo debemos importar. Este se encargará de eliminar en cada training_step algunas neuronas con un determinado `rate`, retropropagar correctamente el error y adaptar los weights correctamente a la hora de evaluación y predicción."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fu3QPqIft5A"
      },
      "source": [
        "#Por ejemplo el modelo primero del notebook, pero con Dropout luego de cada Layer:\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2), #esto lo que esta apagando son algunos pixels de la imagen.\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkAV2qj5j9ub"
      },
      "source": [
        "Preguntenme sobre AlphaDropout y MCDropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb4Vjao5ft5D"
      },
      "source": [
        "## Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwiw3ec6ft5D"
      },
      "source": [
        "Si bien a medida que entrenamos, el costo sobre el training set va decreciendo, cuando comenzamos a sobreajustarlo típicamente el costo en el validation set comenzará a subir. Por esto, una forma de regularizar es ir monitoreando la función de costo sobre el conjunto de validación al final de cada época, y si vemos que luego de ciertas épocas la validación no mejora, simplemente dejamos de entrenar. \n",
        "\n",
        "En keras, early stopping es fácilmente implementable con un Callback, como vimos anteriormente en EXTRAS. Para visualizar a lo que nos referimos, en un tiempo corto, entrenemos la misma red que al principio, pero invirtiendo los roles de entrenamiento y validación. Como el conjunto de validación es pequeño, deberíamos ver que enseguida comenzamos a sobreajustarlo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAI1jHcaft5E"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Ar_xTlft5H"
      },
      "source": [
        "#aquí invierto el rol de los datasets\n",
        "history = model.fit(X_train[:100], y_train[:100], epochs=50,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxCzC2Z3ft5N"
      },
      "source": [
        "Veamos como dan los plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ-FLWDkft5O"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM2VSgWuft5V"
      },
      "source": [
        "Claramente será mejor simplemente dejar de entrenar en la época 20. Esto lo hacemos con el callback de EarlyStopping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC0W6fA7ft5W"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ygNehwwft5a"
      },
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train[:100], y_train[:100], epochs=50,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[early_stopping_cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Xil0u7-Bft5d"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoHvND7Vft5g"
      },
      "source": [
        "Vemos que paró en la época 31, porque pusimos una paciencia de 10 épocas (espera 10 épocas a ver si vuelve a mejorar, en caso que sea una fluctuación. Sin embargo, como pusimos `restore_best_weights=True`, el modelo fiteado debería corresponder al de la época 21:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LFpHjVMaft5h"
      },
      "source": [
        "model.evaluate(X_valid, y_valid, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqZ6NZ30ft5k"
      },
      "source": [
        "Que corresponde a la valid_loss en la época 21."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqVrGNsqlMYU"
      },
      "source": [
        "# Vanishing/Exploding Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06LNh-Rqmupv"
      },
      "source": [
        "Debido a la naturaleza misma de la retro-propagacion, los gradientes son muy inestables y las capas pueden aprender a distintas velocidades.\n",
        "\n",
        "Si los gradientes disminuyen a medida que retrodecemos en la red, las capas mas cercanas al input van a cambiar muchos menos, evitando que la red converga a una solucion. Esto es lo que se llama _vanishing gradient_. La gran consecuencia es que el tiempo de entrenamiento aumenta mucho.\n",
        "\n",
        "Si los gradientes aumentan mas y mas, el algoritmo empieza a diverger. Es lo que se llama _exploding gradient_.\n",
        "\n",
        "Suele ser una conspiracion entre la funcion de activacion y la inicializacion de los pesos que causa que haya saturacion en las capas y por ende el gradiente disminuya mas y mas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCI-q6_y5KMy"
      },
      "source": [
        "def logit(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [1, 1], 'k--')\n",
        "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
        "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
        "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.2, 1.2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGzHSPB1rm3V"
      },
      "source": [
        "## Inicializacion de los pesos (no incluye biases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUHKBPXjvLTx"
      },
      "source": [
        "Una manera de evitar que esto ocurra es inicializar los pesos de manera tal que sucedan dos cosas: que las varianzas de los inputs y outputs de cada neurona sean iguales entre si, y que la varianza del gradiente calculada en la pasada forward y en la backward coincidan.\n",
        "\n",
        "No se pueden garantizar ambas si la capa no tiene igual cantidad de inputs que de outputs pero puede llegarse a un buen compromiso.\n",
        "\n",
        "Aparecen entonces distintas estrategias de inicializacion, a veces asociadas a determinadas funciones de activacion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le7C9vlvnVy-"
      },
      "source": [
        "Inicializar los pesos es obtener la matriz $W$ para cada capa. Se puede samplear al azar, sea de una normal con media 0 y varianza a determinar (distribucion `normal`) o de una uniforme de rango a determinar (`uniform`), o se puede construir una matriz con determinadas caracteristicas (`One`, `Orthogonal`, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbwxJA0YlNem"
      },
      "source": [
        "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuFAqPMXp48-"
      },
      "source": [
        "Definiendo $\\text{fan}$ como:\n",
        "\n",
        "$\\text{fan}_{\\text{avg}}=\\frac{1}{2}(\\text{fan}_{\\text{in}}+\\text{fan}_{\\text{out}})$\n",
        "\n",
        "Con $\\text{fan}_{\\text{in}}$ y $\\text{fan}_{\\text{out}}$ las conexiones de entrada y la cantidad de neurones de la capa, las tres inicializaciones mas relevantes son:\n",
        "\n",
        "*   *Xavier/ Glorot*: $\\sigma^{2}=1/\\text{fan}_{\\text{avg}}$, util para funciones de activacion Identidad, Tanh, Sigmoide y Softmax.\n",
        "*   *He*: $\\sigma^{2}=2/\\text{fan}_{\\text{in}}$, util para ReLU y sus variantes.\n",
        "*   *LeCun*: $\\sigma^{2}=1/\\text{fan}_{\\text{in}}$, util para SELU.\n",
        "\n",
        "Donde se especifica la varianza para la inicializacion gaussiana. La inicializacion uniforme se hace en $[-r,r]$ con $r=\\sqrt{3\\sigma^{2}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UP79Azu5WCT"
      },
      "source": [
        "Por defecto, Keras utiliza inicializacion de Glorot. Si uno quiere cambiar a He, se hace de esta manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrbfz-mL5Xtr"
      },
      "source": [
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61pq3GCX5b1M"
      },
      "source": [
        "O uno propio con:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_sYQDRk5dj6"
      },
      "source": [
        "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
        "                                          distribution='uniform')\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eavej6KrwCWg"
      },
      "source": [
        "## Dying Relus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh_soI6PwkKq"
      },
      "source": [
        "Las ReLUs mueren. LeakyRelUs, RReLUs, etc Funcionan mejor pero overfittean.\n",
        "\n",
        "ELU, SELU: funcionan mejor, no tienen los problemas de las ReLUs pero son mas lentas a la hora de evaluar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qys5YQvotJtJ"
      },
      "source": [
        "El problema de la saturacion se puede evitar utilizando ReLUs (rectified linear unit). Ademas, las ReLUs son tan simples que son muy rapidas para calcular y aceleran el entrenamiento. Sin embargo, las ReLUs tiene su propio problema. Por su definicion misma, la funcion ReLU puede \"matar\" neuronas causando que siempre emitan 0s. Esto sucede cuando los parametros de la neurona dan valores negativos para todo el conjunto de entrenamiento. Por lo tanto, siempre el gradiente vale 0 y el Descenso por Gradiente no cambia nunca los pesos de la neurona. Esto es lo que se conoce como *Dying ReLUs*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz1A1PKdu4S9"
      },
      "source": [
        "Una solucion es que no poner un cero para $x<0$ sino algun valor menor a 1. Esto es lo que se conoce como `LeakyReLU`.\n",
        "\n",
        "$\\text{LeakyReLU}(x)=\\text{Max}(\\alpha x,x)$\n",
        "\n",
        "$\\alpha$ es la pendiente del lado negativo y se suele tomar como 0.01. Ahora el gradiente es no-nulo siempre pero todavia facil de calcular (aunque es no derivable en $x=0$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-NEYg9X5i_c"
      },
      "source": [
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.maximum(alpha*z, z)\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "\n",
        "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
        "plt.grid(True)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.5, 4.2])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_fi1bk_wGco"
      },
      "source": [
        "La `LeakyReLU` se puede incorporar facilmente en Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jGkPvUMwLT_"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWe3hCa2wbPG"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(),\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\",kernel_initializer='GlorotNormal')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkKx4qtbyy0E"
      },
      "source": [
        "keras.layers.LeakyReLU?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJJ8940GyTlz"
      },
      "source": [
        "O en Funcional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm9o3estygcO"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "input_ = keras.layers.Input(shape=[28, 28]) #En este caso es necesario definir el Layer de Inputs\n",
        "flatten = keras.layers.Flatten()(input_)\n",
        "hidden1 = keras.layers.Dense(300)(flatten)#235500\n",
        "activation1 = keras.layers.LeakyReLU(alpha=0.3)(hidden1)\n",
        "hidden2 = keras.layers.Dense(100)(activation1)# \n",
        "activation2 = keras.activations.relu(hidden2,alpha=0.3)#aca la hice a mano usando ReLU, fijense que la sintaxis es un poco distinta porque es una funcion, no una capa\n",
        "output = keras.layers.Dense(10, activation=\"softmax\")(hidden2)#charlemos un poquito la softmax despues\n",
        "model = keras.models.Model(inputs=[input_], outputs=[output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxaevVD5wztI"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns1JCESew003"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6LqAGxXxBP7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkVcZ7k9xLqr"
      },
      "source": [
        "Dos mejoras sobre la LeakyReLU son la randomized LeakyReLU (o RReLU), donde $\\alpha$ se elije al azar en el entrenamiento y se fija a un promedio en la evaluacion, y la parametric LeakyReLU o PReLU, donde $\\alpha$ pasa de hiperparametro a parametro aprendido en la retropropagacion. En general, este ultimo funciona mejor que los otros para datasets grandes pero puede sobreajustar.\n",
        "\n",
        "Se pueden utilizar de la misma manera, como capas,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGrns28w0dAb"
      },
      "source": [
        "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nOmw4D-05aA"
      },
      "source": [
        "**Ejercicio rapido:** Rehaga con PReLU y compare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBjXYdE10h7k"
      },
      "source": [
        "Como funciones, las opciones son estas, con sus parametros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkaXArzc527I"
      },
      "source": [
        "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCS3F8mX007H"
      },
      "source": [
        "Alli vemos ELU (exponential linear unit) y SELU (scaled exponential linear unit). Son otras formas de suavizar la ReLU.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1p0H7PQ-gD8"
      },
      "source": [
        "def elu(z, alpha=1):\n",
        "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [-1, -1], 'k--')\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL4rFL5C-qnd"
      },
      "source": [
        "from scipy.special import erfc\n",
        "\n",
        "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
        "# (see equation 14 in the paper):\n",
        "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
        "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)\n",
        "\n",
        "\n",
        "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
        "    return scale * elu(z, alpha)\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.title(\"SELU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRhoTAXW_Sbd"
      },
      "source": [
        "Tienen ciertas ventajas. En particular, ELU suele ser mas rapida al entrenar y mas lenta al evaluar que ReLU. SELU, bajo ciertas condiciones, suele ser la mejor ya que bajo esas condiciones garantiza que la red se auto-normalize preservando media 0 y varianza 1 para cada capa.\n",
        "\n",
        "Las condiciones son:\n",
        "\n",
        "*   El input debe estar estandarizado con media 0 y varianza 1.\n",
        "*   Los pesos deben estar inicializados con LeCun Gaussiano.\n",
        "*   La red debe ser secuencial, sin dropout, regularizacion, ni conexiones que salteen capas.\n",
        "\n",
        "\n",
        "Un ejemplo de esto ultimo. Inicializemos una red de 1000 capas de 100 neuronas, todo fully connected, alimentados por un input X de 500 instancias de 100 features. Al aplicar SELU sucesivamente vemos que los sucesivos outputs mantienen la distribucion normal.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZON8_MbAN4k"
      },
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.normal(size=(500, 100)) # standardized inputs\n",
        "for layer in range(1000):\n",
        "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
        "    X = selu(np.dot(X, W))\n",
        "    means = np.mean(X, axis=0).mean()\n",
        "    stds = np.std(X, axis=0).mean()\n",
        "    if layer % 100 == 0:\n",
        "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3mZYSI4A3Ov"
      },
      "source": [
        "Apliquemos esto de vuelta a Fashion_MINST pero con una arquitectura muy profunda entrenada pocas veces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAJEDappBE3G"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
        "                             kernel_initializer=\"lecun_normal\"))\n",
        "for layer in range(99):\n",
        "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
        "                                 kernel_initializer=\"lecun_normal\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e79nMfMBhNO"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MJC1Nk5Boer"
      },
      "source": [
        "Antes de entrenar, escaleamos usando el conjunto de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g333IanqBj1i"
      },
      "source": [
        "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
        "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
        "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
        "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
        "X_test_scaled = (X_test - pixel_means) / pixel_stds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRfZnscCBsNi"
      },
      "source": [
        "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-67XBmGCeMg"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eOnL9uRBvyF"
      },
      "source": [
        "**Ejercicio rapido:** Rehaga esto con ReLU y verifique en efecto tiene gradientes que desaparecen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbSneeITxgbK"
      },
      "source": [
        "## Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0ChEu1GC2ws"
      },
      "source": [
        "Esto es bastante importante en implementaciones profesionales, lo podemos charlar despues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHeASkolygVm"
      },
      "source": [
        "## GradientClipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWDvfpCMCt3J"
      },
      "source": [
        "Se suele utilizar cuando BN no es demasiado apta (por ejemplo RNN)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bostTvdiC266"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn-CCGOumrj8"
      },
      "source": [
        "# Optimizadores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o_GcI93bC3-"
      },
      "source": [
        "## Distintos normalizadores:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng3sYRkPEGop"
      },
      "source": [
        "El optimizador es una parte fundamental del entrenamiento. Un buen optimizador debe garantizar convergencia y velocidad. En general, SGD y sus modificaicones van a converger adecuamente pero con problemas de velocidad. Los metodos adaptativos van ser mas veloces, pero pueden tener problemas de convergencia o de generalizacion.\n",
        "\n",
        "Veamos las opciones que tiene Keras:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZa4rHS-CyfV"
      },
      "source": [
        "[name for name in dir(keras.optimizers) if (not name.startswith(\"_\") and name[0].isupper())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNghDRyGTkM9"
      },
      "source": [
        "En el viejo y querido SGD, Keras nos ofrece la opcion de Momentum Optimization\n",
        "\n",
        "$\\vec{m}\\rightarrow \\beta \\vec{m} -\\eta \\nabla_{\\vec{\\theta}} J(\\vec{\\theta}) $\n",
        "\n",
        "$\\vec{\\theta} \\rightarrow \\vec{\\theta} + \\vec{m}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE-V0uO7Enhx"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHaSmklmUSk9"
      },
      "source": [
        "Y el gradiente acelerado de Nesterov:\n",
        "\n",
        "\n",
        "$\\vec{m}\\rightarrow \\beta \\vec{m} -\\eta \\nabla_{\\vec{\\theta}} J(\\vec{\\theta}+\\beta\\vec{m}) $\n",
        "\n",
        "$\\vec{\\theta} \\rightarrow \\vec{\\theta} + \\vec{m}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCvNf-RxUWdS"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WactckRfUuTT"
      },
      "source": [
        "Tambien hay otros optimizadores. Estan los optimizadores adaptativos, que van cambiando el learning rate (no el hiperparametro $\\eta$ si no a lo que multiplica este hiperparametro) a medida que van iterando. Debido a esto, el hiperparametro $\\eta$ no es tan importante, y puede no ser tuneado demasiado.\n",
        "\n",
        "El `AdaGrad` es el primer ejemplo de eso, pero evoluciona demasiado rapido y no es conveniente para Redes Neuronales.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZjWBXC8W-ru"
      },
      "source": [
        "optimizer = keras.optimizers.Adagrad(lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjaBMHefYOhz"
      },
      "source": [
        "El `RMSProp` es un AdaGrad mejorado en ese sentido, introduciendo un decaimiento exponencial controlado por el parametro $\\rho$ (tipicamente 0.9) para evitar la sobreadaptacion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIMOM3BrYg-j"
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDinTVTXYodK"
      },
      "source": [
        "Los mas populares son `Adam`,`Adamax` y `Nadam`.\n",
        "\n",
        "`Adam` combina las estrategias de `Momentum Optimization` y de `RMSProp` , con $\\beta_{1}$ controlando lo primero y $\\beta_{2}$ lo segundo. El parametro $\\epsilon$ esta para suavizar y evitar ceros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRgtezgFZUBB"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSfLDsRYZ9Bf"
      },
      "source": [
        "`Adamax` es una version mas estable de `Adam` pero que suele funcionar peor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2ntDPRLaEqT"
      },
      "source": [
        "optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_fAQ1ZCaFCj"
      },
      "source": [
        "`Nadam` utiliza `NAG` en vez de `Momentum Optimization`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWOP1pfJaQEW"
      },
      "source": [
        "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMAYZk88aBU0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_DUVnD2bFGb"
      },
      "source": [
        "## El learning rate:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS1bgYzhbHy5"
      },
      "source": [
        "Tunear el learning rate puede ser un problema. Existen varias estrategias para hacerlo de manera un poco mas eficiente.\n",
        "\n",
        "La primera es elegir el learning rate en un pre-entrenamiento, en el que se sube el learning rate a medida que se avanza. Luego, se grafica la perdida en funcion del learning rate y se elige el optimo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkV9NTI_b-sA"
      },
      "source": [
        "Otra opcion es utilizar un `Learning Rate Scheduling` en el que se disminuye gradualmente el learning rate para asegurar convergencia.\n",
        "\n",
        "Hay varios tipos de scheduling. En particular, SGD tiene incorporado `Power Scheduling`\n",
        "\n",
        "$\\eta = \\frac{\\eta_{0}}{(1+\\text{epoch}/s)^{c}}$\n",
        "\n",
        "SGD asume c = 1 y s = 1/decay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp5r5ksdcFC_"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyv1qRaZdJC9"
      },
      "source": [
        "Otra opciones en el `Exponential Scheduling` que reduce mas brutalmente el decaimiento a cada paso:\n",
        "\n",
        "$\\eta = \\eta_{0}\\cdot0.1^{\\text{epoch}/s}$\n",
        "\n",
        "Para armarlo, uno puede incorporarlo como `Callback`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNzH5qM3eF4H"
      },
      "source": [
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1**(epoch / 20)\n",
        "\n",
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1**(epoch / s)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duMMb2bneRtz"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "n_epochs = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EZFIQYJeSZd"
      },
      "source": [
        "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid_scaled, y_valid),\n",
        "                    callbacks=[lr_scheduler])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}