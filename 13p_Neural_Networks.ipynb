{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "13p_Neural_Networks",
      "provenance": [],
      "collapsed_sections": [
        "jojgQMNjYFQu",
        "TBYZ4zFYYHDG"
      ]
    },
    "kernelspec": {
      "display_name": "Python [conda env:tf]",
      "language": "python",
      "name": "conda-env-tf-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC18EFXdSqmd"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#The stars\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuLqMAEqS5fa"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-BToGrRTCK1"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTr73m2gTDu2"
      },
      "source": [
        "tf.config.list_physical_devices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoNOblmHX1XY"
      },
      "source": [
        "# Datos: Clasificacion de ropa (Fashion_MNIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PMm5jBHq_7a"
      },
      "source": [
        "Asi como sklearn trae utilidades para cargar datasets estandar, keras también trae. En general, keras puede aceptar datasets en forma de Numpy Arrays (como sklearn), pero también trae una clase Dataset que esta optimizada para cargar datos (incluso si son mas grandes que la memoria RAM del equipo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiVqgSJhTeXp"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy7mdQSCgIWs"
      },
      "source": [
        "Veamos el shape y el tipo de datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjrBwOsAgKde"
      },
      "source": [
        "print(X_train_full.shape,y_train_full.shape)\n",
        "print(X_train_full.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42tIWD0xgoGd"
      },
      "source": [
        "Tenemos la imagen en (28,28) y cada pixel es un entero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIus71mjtqxf"
      },
      "source": [
        "En Datasets grandes, CrossValidation puede ser muy costoso, por lo cual se recomienda separar un conjunton de validación aparte del training. Esto hacemos a continuación.\n",
        "\n",
        "También normalizamos los píxeles (que van de 0 a 255) para que estén entre 0 y 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cB3KYyaUnzP"
      },
      "source": [
        "#Separo en entrenamiento y validacion, y normalizo los pixeles\n",
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I16qmEbDVnsa"
      },
      "source": [
        "print(X_train_full.shape)\n",
        "\n",
        "plt.imshow(X_train[0], cmap='binary', interpolation='bicubic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nY8YQ_0uEPg"
      },
      "source": [
        "Los targets son numericos, del 0 al 9. Podemos guardar las etiquetas asi nos es mas facil analizar que tan bien o mal funciona nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNkEq7xEU1YD"
      },
      "source": [
        "class_names = [\"Remera/top\", \"Pantalones\", \"Pullover\", \"Vestido\", \"Abrigo\",\n",
        "               \"Sandalias\", \"Camisa\", \"Zapatillas\", \"Bolso\", \"Bota\"]\n",
        "\n",
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_staGAxY9yH"
      },
      "source": [
        "# Definir un modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsfK9opSuPfg"
      },
      "source": [
        "Hay tres formas de definir modelos:\n",
        "* Una es utilizando la clase de ``keras.models.Sequential``, que es apta para modelos donde solamente Layers adyacentes se encuentran conectados entre sí. \n",
        "* Otra es API funcional, que provee mucha mas flexibilidad, ya que se puede construir cualquier clase de red dirigida acíclica. \n",
        "* La ultima forma, es con SubClassing, es decir definiendo nuestras propias clases que hereden de las definidas en Keras, lo cual nos permite extender Keras para nuestras necesidades específicas, aunque no es recomendable a menos que realmente sepamos lo que estamos haciendo.\n",
        "\n",
        "Si bien ahora quizás les sea mas sencillo utilizar la API secuencial, aprender a utilizar la API funcional es casi igual de fácil y provee mas flexibilidad, asi que es la forma que recomiendo de aprender a usar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36PryIS5X_Oc"
      },
      "source": [
        "## API Secuencial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZXmZK8yXF4u"
      },
      "source": [
        "### Modo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3lSxEA-vSsr"
      },
      "source": [
        "Podemos crear un modelo, e ir añadiendo Layers sobre la marcha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvgn-wSlYV89"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnwg1ZjvXLZE"
      },
      "source": [
        "#Creo un modelo secuencial\n",
        "model = keras.models.Sequential()\n",
        "#Y le voy agregando layers\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28])) #matriz->vector (como np.ravel)\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\")) #hidden 1\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\")) #hidden 2\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\")) #output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sGF3s9xsHpH"
      },
      "source": [
        "Las funciones de activacion son muy importantes a la hora de usar DNN. Son las que agregan no-linearidad a la situacion.\n",
        "\n",
        "En las capas internas, la mas usual es ReLU, una funcion que es 0 si $z\\leq0$ y z si $z>0$. Hay otras opciones que suelen ser mejores, en particular para datasets grandes y con ciertas condiciones, pero ReLU tiene mucha ventaja en velocidad y optimizacion de librerias.\n",
        "\n",
        "En la capa externa, va a depender del problema.\n",
        "\n",
        "\n",
        "*   Para clasificacion, se utilizan sigmoide o softmax.\n",
        "*   Para regresion, si no tenemos ninguna restriccion no utilizamos ninguna funcion de activacion. Si queremos $y\\geq 0 $, podemos usar una funcion ReLu o una version mas suavizada llamada Softplus. Si queremos $a\\leq y \\leq b$, se puede utilizar una funcion sigmoide o tangente hiperbolica y re-escalearla.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXkcO1-B5rmc"
      },
      "source": [
        "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jojgQMNjYFQu"
      },
      "source": [
        "### Modo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb17UrHJvTv8"
      },
      "source": [
        "También podemos directamente pasar la lista de Layers al constructor del Modelo ``Sequential``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63vXIjKCYTOm"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_2k6HdfXlr5"
      },
      "source": [
        "#Sino directamente le paso una lista al crearlo:\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBYZ4zFYYHDG"
      },
      "source": [
        "## API Funcional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gWy2OctvbZF"
      },
      "source": [
        "La API funcional funciona definiendo Layers y utilizandolo como si fueran \"funciones\" que se aplican a los outputs de otros Layers. Keras irá rastreando estas conecciones, y luego generará un modelo entre el Layer de Inputs, y el de Outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BdBiQp1YXiJ"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-1uPqZmYIeh"
      },
      "source": [
        "input_ = keras.layers.Input(shape=[28, 28]) #En este caso es necesario definir el Layer de Inputs\n",
        "flatten = keras.layers.Flatten()(input_)\n",
        "hidden1 = keras.layers.Dense(300, activation=\"relu\")(flatten)\n",
        "hidden2 = keras.layers.Dense(100, activation=\"relu\")(hidden1)\n",
        "output = keras.layers.Dense(10, activation=\"softmax\")(hidden2)#charlemos un poquito la softmax despues\n",
        "model = keras.models.Model(inputs=[input_], outputs=[output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OwGQ8puZJmI"
      },
      "source": [
        "# Compilar y visualizar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1n6UL-ovs4G"
      },
      "source": [
        "Una vez que hemos definido nuestro modelo, es necesario que lo \"compilemos\", entonces Keras creará el gráfico computacional en TensorFlow de acuerdo a como lo hemos definido. Al compilar necesitamos especificar la funcion de perdida, el optimizador y las distintas metricas que querramos almacenar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVncsjmrZMbz"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vOu_s_zhNel"
      },
      "source": [
        "La funcion de perdida define el problema y esta intimamente relacionada con la funcion de activacion de la output layer. Para clasificacion, tenemos algunas sutilezas.\n",
        "\n",
        "Si tenemos un problema de dos clases, tenemos dos opciones:\n",
        "\n",
        "* Utilizamos una unica neurona en la output layer. Esa neurona modela la asignacion a clase 1 o clase 2. La funcion de activacion sera entonces una sigmoide y la funcion de perdida la binary_cross_entropy.\n",
        "* Utilizamos dos neuronas en la output layer. Cada una modela la probabilidad de pertenecer a una determinada clase. Dado que deben sumar 1, utilizamos la funcion de activacion softmax.  La funcion de perdida sera categorica. Pero aun hay mas. Si utilizamos la representacion $y={0,1}$, utilizamos sparse_categorical_cross_entropy. Si utilizamos $y={[0,1],[1,0]}$, utilizamos simplemente categorical_cross_entropy. Se puede pasar de sparse a categorical con `tensorflow.keras.utils.to_categorical` y de categorical a sparse con `np.argmin()`.\n",
        "\n",
        "Cuando tenemos N clases, tenemos lo siguiente:\n",
        "\n",
        "*   Si tenemos un problema MultiClase, en el que queremos asignar la medicion a una de las clases, utilizamos funcion de activacion softmax y categorical_cross_entropy o sparse_categorical_cross_entropy dependiendo de como sea $y$.\n",
        "*   Si tenemos N clasificaciones en donde solo queremos decir si pertenece o no a determinada clase, utilizamos funcion de activacion sigmoide y cateogircal_cross_entropy (con el one hot encoding).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zARoy-X0v66Q"
      },
      "source": [
        "Podemos acceder a los diferentes Layers, mediante el atributo ``.layers``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaYiz7qVZqSw"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYy-e7pzwBHz"
      },
      "source": [
        "Y También podemos imprimir un resumen de nuestro modelo, asi como un lindo grafico de el:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LXinQM0ZquT"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HugLm6sZvmx"
      },
      "source": [
        "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcFeksoHwHbQ"
      },
      "source": [
        "Cada layer tiene atributos a los que podemos acceder, como el nombre, y métodos que nos devuelven, por ejemplo, los pesos.\n",
        "\n",
        "Vemos que por default inicializa los pesos de forma aleatoria entre 0 y 1, y los bias a cero. Esto se puede cambiar pasando un ``initializer=``al compilar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Js3Y9CgZ17S"
      },
      "source": [
        "hidden1 = model.layers[1]\n",
        "print(hidden1.name)\n",
        "weights, biases = hidden1.get_weights()\n",
        "print(weights.shape)\n",
        "print(weights[0], biases[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRf0_MB6ZdBX"
      },
      "source": [
        "# Entrenar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRSYAHruwcjM"
      },
      "source": [
        "Entrenar es tan facil como hacer un ``.fit``, donde podemos especificar muchas cosas como épocas, datos de validación (que evaluará al final de cada época), métricas, etc.\n",
        "\n",
        "El método devuelve un objeto de Historia, con toda la información del entrenamiento (en forma de diccionarios), que podremos utilizar para analizar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL9AKbgfZeQe"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubNbhHnEwwCq"
      },
      "source": [
        "Examinemos el diccionario de parámetros del fit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IITIPuw9aATd"
      },
      "source": [
        "history.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgFx-tzNaGxG"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tg1GkrJw11Y"
      },
      "source": [
        "Podemos usar el atributo \"history\" que nos da un diccionario, para plotear las métricas que usamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ksDH-nWaLfX"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zMsHUuRZgBj"
      },
      "source": [
        "# Evaluar y predecir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0RQHI_GxEw0"
      },
      "source": [
        "Evaluar en un conjunto de Test, o hacer predicciones, es sumamente fácil:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYcvFqohZhVv"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npHqXo7aSUs"
      },
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lysdDpXXaXFY"
      },
      "source": [
        "y_pred = model.predict_classes(X_new)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ybn92SmonYj"
      },
      "source": [
        "y_pred = np.argmax(y_proba, axis=-1)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVDFbrnOaXN6"
      },
      "source": [
        "np.array(class_names)[y_pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffbf9KFtadJY"
      },
      "source": [
        "plt.figure(figsize=(7.2, 2.4))\n",
        "for index, image in enumerate(X_new):\n",
        "    plt.subplot(1, 3, index + 1)\n",
        "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[y_test[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpzyBn70ZigS"
      },
      "source": [
        "# Guardar y Cargar Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S__NXAnsxI_5"
      },
      "source": [
        "Guardar modelos es tan sencillo como hacer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDg65TpBZkkH"
      },
      "source": [
        "model.save(\"my_keras_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyTFAQroxL2B"
      },
      "source": [
        "Que luego se puede cargar (por ejemplo en otra computadora, con otro script, para hacer predicciones) usando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT-6iPoPauFY"
      },
      "source": [
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrejS892xUQ5"
      },
      "source": [
        "También podemos guardar solamente los weights de los layers. Pero en este caso deberemos, antes de cargarlos, definir el mismo modelo que usamos y recien entonces cargar los weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd0dte83axdC"
      },
      "source": [
        "model.save_weights(\"my_keras_weights.ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCEBS3enaz0H"
      },
      "source": [
        "model.load_weights(\"my_keras_weights.ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhizHlVnkjKR"
      },
      "source": [
        "# Regresión: California Housing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdlNX14NxgdO"
      },
      "source": [
        "Veamos como se desempeña para hacer regresión con el dataset de california housing que utilizamos en la guía de ejercicios Datasets. Importamos los datos, y los escaleamos usando StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_g1GZM9km8Q"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaLKkccNxptl"
      },
      "source": [
        "El modelo que definimos tiene como ultimo layer una funcion lineal, sin ninguna funcion de activación. (Es como hacer regresión lineal, pero fiteando unidades ReLu como funciones de base)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFDtfmzxkyls"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogy8zfnUk9lJ"
      },
      "source": [
        "pd.DataFrame(history.history).plot()\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5fQBF1XlAYM"
      },
      "source": [
        "## Ventajas de la API funcional: Modelos no secuenciales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKCoZmChx5rf"
      },
      "source": [
        "Aqui vemos un ejemplo de como podemos usar la API funcional para hacer modelos mas complejos. En el modelo [Wide and Deep](https://https://ai.google/research/pubs/pub45413) se conectan los inputs directamente a los outputs, ademas de pasando por una red profunda de dos capas. De esta manera, el modelo puede aprender features mas abstractos de los datos, y a la vez tomarlos directamente sin distorisión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emM5moUylGFE"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model = keras.models.Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJBY5y7fpWc2"
      },
      "source": [
        "keras.utils.plot_model(model, \"wide_and_deep_model.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmapksBIlNWA"
      },
      "source": [
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hjgiSLzbRo1"
      },
      "source": [
        "# EXTRAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0UQUJJOyVu0"
      },
      "source": [
        "Callbacks son una manera de hacer cosas durante el entrenamiento. Son utiles en caso que el modelo tarde mucho en fitear. Por ejemplo, podemos guardar el modelo al fin de cada epoca (por si se nos apaga la maquina en el medio del training), o implementar un regularizador EarlyStopping y dejar de fitear si el validation score no mejora, o enviarnos un mail cuando termine el entrenamiento con los resultados, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW0PRpE-a10X"
      },
      "source": [
        "## Callbacks: Guardar durante entrenamiento (checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtJ5vPqplluY"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nY_ql6qa5ph"
      },
      "source": [
        " checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
        "\n",
        " history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                     callbacks=[checkpoint_cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3OtrP40p__9"
      },
      "source": [
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpZxyWWqqCPQ"
      },
      "source": [
        "model.evaluate(X_valid,y_valid) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03oEvTcQbVcL"
      },
      "source": [
        "## Callbacks: Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWppr7b4lvms"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3bXGXaRbXIt"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmuTxeNCl6nc"
      },
      "source": [
        "## Busqueda de Hiperparámetros: Sklearn integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5nqSWlNytYA"
      },
      "source": [
        "Y lo mejor de todo, tiene un wrapper que nos permite integrarlo con ScikitLearn. Sklearn lo reconocerá el modelo como uno más de los suyos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db6zYIGMl5_B"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxMV0o_vmBDG"
      },
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vaBNv0-mWV3",
        "scrolled": false
      },
      "source": [
        "keras_reg.fit(X_train, y_train, epochs=30,\n",
        "              validation_data=(X_valid, y_valid),\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_sHKiYYy7O7"
      },
      "source": [
        "El modelo tiene los métodos usuales de un estimador de sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrpGmS1FmcSC"
      },
      "source": [
        "mse_test = keras_reg.score(X_test, y_test)\n",
        "y_pred = keras_reg.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUwX9oDNy2HY"
      },
      "source": [
        "Por ejemplo, podemos usarlo para buscar hiperparámetros con RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkr2t3Ivmnfz"
      },
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_valid, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxACOnKUft4e"
      },
      "source": [
        "Y accedemos a los resultados del CV como siempre hacemos en sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqry-VbMmtEB"
      },
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghtQ2174mukj"
      },
      "source": [
        "rnd_search_cv.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-JpwTtQmv-R"
      },
      "source": [
        "rnd_search_cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNPoFVMRmxQP"
      },
      "source": [
        "rnd_search_cv.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr3DHzRBmzdd"
      },
      "source": [
        "model = rnd_search_cv.best_estimator_.model\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajW_nTpNft4y"
      },
      "source": [
        "### Otras librerías para Optimización:\n",
        "\n",
        "Hay muchas, que hacen cosas un poco mejores que buscar en una grilla o de forma Random. Acá selecciono tres a dedo (en el Gerón pueden ver más opcionaes también):\n",
        "\n",
        " * [Keras Tuner](https://keras-team.github.io/keras-tuner/): Es parte del ecosistema de Keras, por lo que está optimizado para la versión de tensorflor: tf.keras, y tiene herramientas de visualización útiles.\n",
        " * [Scikit-Optimize](https://scikit-optimize.github.io/): Similar a sklearn para algoritmos de aprendizaje automatizado, es esta librería para algoritmos de optimización. En particular `BayesSearchCV` hace optimización Bayesiana con una interfaz similar al `GridSearchCV`.\n",
        " * [Sklearn-Deap](https://github.com/rsteca/sklearn-deap): Esta pretende imitar al `GridSearchCV` de sklearn para buscar hiperparámetros, pero utilizando algoritmos evolutivos en su lugar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqp8IMn_ft4y"
      },
      "source": [
        "# Regularización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yaDFBJrft4z"
      },
      "source": [
        "Sabemos como hacer más flexible nuestra red: Aumentamos el numero de Layers o de Neuronas por Layer. Pero, cómo la regularizamos para prevenir overfitting?\n",
        "\n",
        "Acá presentamos tres formas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO6qM7DAft40"
      },
      "source": [
        "## Weight-decay: Regularización $\\ell_2$ ó $\\ell_1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxOlPfO8ft41"
      },
      "source": [
        "Similar a los métodos de regularización Ridge y Lasso para modelos lineales, que añadían un término a la función de costo proporcional a la norma $\\ell_2$ o $\\ell_1$ del vector de pesos, uno puede hacer lo mismo en la red neuronal. \n",
        "\n",
        "Implementar weight decay en una red neuronal en keras, es tan simple como añadir el argumento `kernel_regularizer=` en los layers que queramos regularizar. Los regularizadores $\\ell_2$ o $\\ell_1$ los podemos encontrar en `keras.regularizers.l2()` y  `keras.regularizers.l1()`, o incluso un ElasticNet (combinacion de ambos) en `keras.regularizers.l1_l2()`. Al inicializarlos les deberíamos pasar como argumento el peso que tendrá estos términos (equivalente al `alpha` en sklearn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfM7HylOft42"
      },
      "source": [
        "#por ejemplo para definir un layer denso ReLy de 100 neuronas y regularización l2 (~Ridge) con alpha=0.01:\n",
        "layer = keras.layers.Dense(100,\n",
        "                          activation='relu',\n",
        "                          kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGtR3s41ft45"
      },
      "source": [
        "Gerón ademas nos da un truco para ahorrarnos escribir esto en cada layer, y cualquier hiperparámetro que se repita: Crear un wrapper a dicho layer con la función de Python `functools.partial`. Esta crea wrappers sencillos a funciones/objetos, que pasan hiperparámetros default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNJCOTDIft46"
      },
      "source": [
        "#Por ejemplo el modelo primero del notebook, pero regularizado:\n",
        "from functools import partial\n",
        "\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                          activation='relu',\n",
        "                          kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(300),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wveIngKOft4-"
      },
      "source": [
        "## Dropout Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYuzXI7Hft4_"
      },
      "source": [
        "Una forma muy popular de hacer regularización fue propuesta en dos papers de 2012 y 2014: En cada paso de entrenamiento, algunas neuronas de la red se \"apagan\" con una probabilidad $p$. Esto solo se hace durante entrenamiento (no durante test o en predicciones). La idea de fondo es que estamos forzando a la red a que no dependa fuertemente de ninguna neurona en particular, sino que sería dar mucha importancia a algún feature en particular, sino que la forzamos a dar resultados aún cuando ciertas neuronas se apagan.\n",
        "\n",
        "En keras, esto ya ha sido implementado como un Layer, así que solamente lo debemos importar. Este se encargará de eliminar en cada training_step algunas neuronas con un determinado `rate`, retropropagar correctamente el error y adaptar los weights correctamente a la hora de evaluación y predicción."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fu3QPqIft5A"
      },
      "source": [
        "#Por ejemplo el modelo primero del notebook, pero con Dropout luego de cada Layer:\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2), #esto lo que esta apagando son algunos pixels de la imagen.\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb4Vjao5ft5D"
      },
      "source": [
        "## Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwiw3ec6ft5D"
      },
      "source": [
        "Si bien a medida que entrenamos, el costo sobre el training set va decreciendo, cuando comenzamos a sobreajustarlo típicamente el costo en el validation set comenzará a subir. Por esto, una forma de regularizar es ir monitoreando la función de costo sobre el conjunto de validación al final de cada época, y si vemos que luego de ciertas épocas la validación no mejora, simplemente dejamos de entrenar. \n",
        "\n",
        "En keras, early stopping es fácilmente implementable con un Callback, como vimos anteriormente en EXTRAS. Para visualizar a lo que nos referimos, en un tiempo corto, entrenemos la misma red que al principio, pero invirtiendo los roles de entrenamiento y validación. Como el conjunto de validación es pequeño, deberíamos ver que enseguida comenzamos a sobreajustarlo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAI1jHcaft5E"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Ar_xTlft5H"
      },
      "source": [
        "#aquí invierto el rol de los datasets\n",
        "history = model.fit(X_train[:100], y_train[:100], epochs=50,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxCzC2Z3ft5N"
      },
      "source": [
        "Veamos como dan los plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ-FLWDkft5O"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM2VSgWuft5V"
      },
      "source": [
        "Claramente será mejor simplemente dejar de entrenar en la época 20. Esto lo hacemos con el callback de EarlyStopping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC0W6fA7ft5W"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ygNehwwft5a"
      },
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train[:100], y_train[:100], epochs=50,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[early_stopping_cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Xil0u7-Bft5d"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoHvND7Vft5g"
      },
      "source": [
        "Vemos que paró en la época 31, porque pusimos una paciencia de 10 épocas (espera 10 épocas a ver si vuelve a mejorar, en caso que sea una fluctuación. Sin embargo, como pusimos `restore_best_weights=True`, el modelo fiteado debería corresponder al de la época 21:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LFpHjVMaft5h"
      },
      "source": [
        "model.evaluate(X_valid, y_valid, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqZ6NZ30ft5k"
      },
      "source": [
        "Que corresponde a la valid_loss en la época 21."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqVrGNsqlMYU"
      },
      "source": [
        "# Vanishing/Exploding Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06LNh-Rqmupv"
      },
      "source": [
        "Debido a la naturaleza misma de la retro-propagacion, los gradientes son muy inestables y las capas pueden aprender a distintas velocidades.\n",
        "\n",
        "Si los gradientes disminuyen a medida que retrodecemos en la red, las capas mas cercanas al input van a cambiar muchos menos, evitando que la red converga a una solucion. Esto es lo que se llama _vanishing gradient_.\n",
        "\n",
        "Si los gradientes aumentan mas y mas, el algoritmo empieza a diverger. Es lo que se llama _exploding gradient_.\n",
        "\n",
        "Suele ser una conspiracion entre la funcion de activacion y la inicializacion de los pesos que causa que haya saturacion en las capas y por ende el gradiente disminuya mas y mas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCI-q6_y5KMy"
      },
      "source": [
        "def logit(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [1, 1], 'k--')\n",
        "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
        "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
        "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.2, 1.2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGzHSPB1rm3V"
      },
      "source": [
        "## Inicializacion de los pesos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUHKBPXjvLTx"
      },
      "source": [
        "Una manera de evitar que esto ocurra es inicializar los pesos de manera tal que sucedan dos cosas: que las varianzas de los inputs y outputs de cada neurona sean iguales entre si, y que la varianza del gradiente calculada en la pasada forward y en la backward coincidan.\n",
        "\n",
        "No se pueden garantizar ambas si la capa no tiene igual cantidad de inputs que de outputs pero puede llegarse a un buen compromiso.\n",
        "\n",
        "Aparecen entonces distintas estrategias de inicializacion, a veces asociadas a determinadas funciones de activacion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbwxJA0YlNem"
      },
      "source": [
        "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UP79Azu5WCT"
      },
      "source": [
        "Y esto se incorpora de esta manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrbfz-mL5Xtr"
      },
      "source": [
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61pq3GCX5b1M"
      },
      "source": [
        "O uno propio con:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_sYQDRk5dj6"
      },
      "source": [
        "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
        "                                          distribution='uniform')\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eavej6KrwCWg"
      },
      "source": [
        "## Dying Relus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh_soI6PwkKq"
      },
      "source": [
        "Las ReLUs mueren. LeakyRelUs, RReLUs, etc Funcionan mejor pero overfittean.\n",
        "\n",
        "ELU, SELU: funcionan mejor, no tienen los problemas de las ReLUs pero son mas lentas a la hora de evaluar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-NEYg9X5i_c"
      },
      "source": [
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.maximum(alpha*z, z)\n",
        "\n",
        "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
        "plt.grid(True)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.5, 4.2])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkaXArzc527I"
      },
      "source": [
        "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkuwCmeq58w8"
      },
      "source": [
        "[m for m in dir(keras.layers) if \"relu\" in m.lower()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbSneeITxgbK"
      },
      "source": [
        "## Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wezL1u7Gxme3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHeASkolygVm"
      },
      "source": [
        "## GradientClipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HMIibKtyiPo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8dciWUi5GCv"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn-CCGOumrj8"
      },
      "source": [
        "# Optimizadores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKyMqhPLms1z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}